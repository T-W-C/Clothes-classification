{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt of clothes classification using the MNIST \n",
    "Keras will be utilised to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING ALL OF THE IMPORTS\n",
    "\n",
    "# __future__ allows for compatability of certain commands\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Fashion MNIST' dataset is utilised https://www.kaggle.com/zalando-research/fashionmnist\n",
    "Contains 70k fashion based images of 10 different categories - 28x28 pixels\n",
    "stored within the keras dataset library, with specified train, test data:\n",
    "6:1 split between train:test data \n",
    "\n",
    "all, train_images, train_labels, test_images, test_labels are all arrays. \n",
    "\n",
    "The train set is defined by: (train_images, train_labels)\n",
    "and the test set defined by: (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_fashion_data = keras.datasets.fashion_mnist\n",
    "(train_images, train_types), (test_images, test_types) = MNIST_fashion_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info on the data:\n",
    "\n",
    "Image dataset contains the 28x28 monoscale images. \n",
    "Each image belongs to a type, each type is associated with a number, 0 to 9:\n",
    "0: tshirt, 1: trouser, 2: jumper, 3: dress, 4: coat, 5: sandal, 6: shirt, 7: sneaker, 8: bag, 9: ankle boot\n",
    "\n",
    "There are a total of 70,000 images, 60,000 training images, 10,000 test. So respectivelly there are 60,000 training_types and 10,000 test_types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing images shape: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing images shape: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing images shape: ')\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training images shape: ')\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types will match the amount of elements as the first element of the shape for both testing and training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.imshow(train_images[200], cmap=\"gray\",vmin=0, vmax=255)\n",
    "plot.grid(False)\n",
    "plot.colorbar()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simplify the model and speed up processing time, it is important the data is remapped into floats between 0 and 1.  \n",
    "The maximum possible value within the dataset is 255.  \n",
    "Thus all data is divided by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the data the numerical type 0-9 is to be mapped to strings representing each clothing type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_types = ['T-shirt', 'Trouser', 'Jumper', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle-boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers within the nerual network need to be defined.  \n",
    "For classification of the clothes, I will only define one hidden layer which then outputs to the output layer representing each of the classes.  \n",
    "A densely connected NN is whereby the neurons in the input are connected to every single \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the format image, a 2D array 28x28 is flattened, to be represented as a 1D 784 long array.\n",
    "modelInput = [keras.layers.Flatten(input_shape=(28, 28)), \n",
    "             keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "             keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "             ]\n",
    "model = keras.Sequential(modelInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings define the way the neural network behaves.  \n",
    "The Optimizer dictates how the model is updated based upon the data and loss function.  \n",
    "The loss function is how loss is calculated for each predicted output data point with respect to the expected outcome. It is aimed to be minimised during the training process.  \n",
    "Metrics determine how the training is to be monitored, what defines its success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training the training dataset for the images and labels are utilised.  \n",
    "The amount of epochs determines the amount of iterations used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.4967 - acc: 0.8252\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.3767 - acc: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafc75d04e0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_types, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.3382 - acc: 0.8760\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.3127 - acc: 0.8862\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2946 - acc: 0.8916\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2801 - acc: 0.8961\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.2679 - acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafd8158048>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_types, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1316 - acc: 0.9499\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1298 - acc: 0.9506\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1272 - acc: 0.9522\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.1262 - acc: 0.9524\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1209 - acc: 0.9548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafb0e5c668>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,train_types, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get an indication of the models accuracy - outside the training data, the model can be evaluated against the test data.  \n",
    "The difference between the accuracy of the trained model and the test data evaluation is due to overfitting to the training data. This difference is to be minimised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.4399 - acc: 0.8890\n",
      "accuracy:  0.889\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "loss, acc = model.evaluate(test_images, test_types)\n",
    "print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model achieved an output of 90% accuracy for clothing classification and 87% accuracy when utilising the test data.  \n",
    "Predictions can now be made with the trained model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above variable is an array, of an array of size 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each prediction element contains an array of size 10 with statistical probabilities for what clothes type (class) the image belongs to based upon the models predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes:\n",
      "the values below are floored - so insignificant numbers rounded to 0\n",
      "T-shirt: 99.0 percent likely\n",
      "Trouser: 0.0 percent likely\n",
      "Jumper: 0.0 percent likely\n",
      "Dress: 0.0 percent likely\n",
      "Coat: 0.0 percent likely\n",
      "Sandal: 0.0 percent likely\n",
      "Shirt: 0.0 percent likely\n",
      "Sneaker: 0.0 percent likely\n",
      "Bag: 0.0 percent likely\n",
      "Ankle-boot: 0.0 percent likely\n",
      "\n",
      "THE CORRESPONDING IMAGE TO WHAT WAS PREDICTED AGAINST:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'T-shirt')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "apparel_item = 677\n",
    "pred1 = predictions[apparel_item]\n",
    "print('Predicted classes:')\n",
    "print('the values below are floored - so insignificant numbers rounded to 0')\n",
    "for i in range(len(pred1)):\n",
    "    formatted_str = [clothing_types[i], pred1[i]*100]\n",
    "\n",
    "    print(\"%s: %s percent likely\" % (clothing_types[i], np.floor(pred1[i]*100)))\n",
    "\n",
    "    \n",
    "print()\n",
    "print('THE CORRESPONDING IMAGE TO WHAT WAS PREDICTED AGAINST:')\n",
    "plot.imshow(test_images[apparel_item], cmap=plot.cm.binary)\n",
    "plot.xlabel(clothing_types[test_types[apparel_item]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted clothing item is ankle-boot based upon the first image in the test data set, this value is obtained through use of np.argmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements or additions to consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use of several models to provide prediction in app\n",
    "    -  Rely on separate datasets - develop models separately, query both models and give the biggest predicted class\n",
    "    OR\n",
    "    - Combine datasets - scale up the Fashion MNIST? Add colour via an external library, combine this dataset with the deepfashion\n",
    "    - Implement removal of background on images taken in app - through inclusion of an external libary\n",
    "    \n",
    "    - Rely on on Fashion MNIST model for clothes classification, Deepfashion for feature prediction... \n",
    "    \n",
    "    - Deepfashion dataset better suited for clothing worn on individuals rather than isolated... \n",
    "    \n",
    "    \n",
    "- Detection of several clothing items in a single image\n",
    "    - User may take the photo of several of items of clothing\n",
    "    \n",
    "    \n",
    "    \n",
    "What are the most useful metrics to gain from graphing of the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-176-daf59249373d>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-176-daf59249373d>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    def plot_value_array()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_images.shape\n",
    "\n",
    "\n",
    "\n",
    "def predicted_type_comparison():\n",
    "    arr = {}\n",
    "    for i in range(len(clothing_types)):\n",
    "        \n",
    "        predicted_type = []\n",
    "        \n",
    "    \n",
    "        for j in range(len(test_types)):\n",
    "            el = np.argmax(predictions[j])\n",
    "#             print(j)\n",
    "            if(test_types[j] == i):\n",
    "#                 print(test_types[j])\n",
    "                predicted_type.append(el)\n",
    "        \n",
    "        arr.update({clothing_types[i]: predicted_type})\n",
    "    \n",
    "    return arr\n",
    "\n",
    "                \n",
    "extract_type()\n",
    "\n",
    "\n",
    "# X axis: all test data points\n",
    "# Y axis: predicted percentage...\n",
    "#  data points - create 10 lines, for each respective clothes type\n",
    "\n",
    "def plot_value_array()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
